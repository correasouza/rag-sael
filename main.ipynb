{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import dotenv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from transformers import logging\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9857eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPEN_API_URL\", \"https://inference.do-ai.run/v1\")\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\", \"true\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\", \"rag-sael\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model=os.getenv(\"OPEN_MODEL\", \"openai-gpt-oss-120b\"),\n",
    "    model_provider=\"openai\",\n",
    "    base_url=os.getenv(\"OPEN_API_URL\", \"https://inference.do-ai.run/v1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec88fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    path=\"docs\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da910c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "tools = [retrieve_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5703fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a book for logic programming. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    "    \"Responda sempre em português, mesmo que a pergunta seja feita em outro idioma.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487373dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model, \n",
    "    tools, \n",
    "    system_prompt=prompt,\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"O que é lógica proposicional segundo a apostila?\",\n",
    "    \"Como a apostila define uma proposição?\",\n",
    "    \"O que são conectivos lógicos e quais são apresentados no material?\",\n",
    "    \"O que é uma tabela-verdade e para que ela é utilizada?\",\n",
    "    \"Como a apostila define tautologia, contradição e contingência?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Lógica proposicional é o ramo da lógica que estuda proposições e as relações entre elas por meio de conectivos lógicos.\",\n",
    "    \"Proposição é toda sentença declarativa que pode ser classificada como verdadeira ou falsa, mas não ambas.\",\n",
    "    \"Conectivos lógicos são operadores que conectam proposições, como negação (¬), conjunção (∧), disjunção (∨), condicional (→) e bicondicional (↔).\",\n",
    "    \"Tabela-verdade é um método utilizado para determinar o valor lógico de proposições compostas a partir dos valores lógicos das proposições simples.\",\n",
    "    \"Tautologia é uma proposição composta que é sempre verdadeira; contradição é sempre falsa; contingência é aquela que pode ser verdadeira ou falsa dependendo dos valores das proposições componentes.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_and_collect_data(query: str, ground_truth: str) -> Dict[str, Any]:\n",
    "    thread_id = str(uuid.uuid4())\n",
    "    \n",
    "    events = list(agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}},\n",
    "        stream_mode=\"values\",\n",
    "    ))\n",
    "    \n",
    "    final_event = events[-1]\n",
    "    answer = final_event[\"messages\"][-1].content\n",
    "\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    contexts = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    return {\n",
    "        \"question\": query,\n",
    "        \"contexts\": contexts,\n",
    "        \"answer\": answer,\n",
    "        \"ground_truth\": ground_truth\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_ragas():\n",
    "    print(\"Executando agent para coletar dados de teste...\")\n",
    "    ragas_data = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"Testando: {query}\")\n",
    "        data_point = run_agent_and_collect_data(query, ground_truths[i])\n",
    "        ragas_data.append(data_point)\n",
    "    \n",
    "    test_dataset = Dataset.from_list(ragas_data)\n",
    "    \n",
    "    print(\"\\nExecutando avaliação RAGAS...\")\n",
    "\n",
    "    eval_llm = ChatOpenAI(\n",
    "        model=os.getenv(\"OPEN_MODEL\", \"openai-gpt-oss-120b\"),\n",
    "        base_url=os.getenv(\"OPEN_API_URL\", \"https://inference.do-ai.run/v1\"),\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    result = evaluate(\n",
    "        test_dataset,\n",
    "        metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    "        llm=eval_llm,\n",
    "        embeddings=embeddings \n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== RESULTADOS RAGAS ===\")\n",
    "    print(result)\n",
    "    \n",
    "    df = result.to_pandas()\n",
    "    print(\"\\nDetalhes por query:\")\n",
    "    print(df)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_result = evaluate_with_ragas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86602ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = ragas_result.to_pandas()\n",
    "\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "available_metrics = [m for m in metrics if m in df_results.columns]\n",
    "mean_scores = [df_results[m].mean() for m in available_metrics]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(available_metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79665bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 13,\n",
    "    'axes.labelsize': 11,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 14,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.linewidth': 0.8,\n",
    "    'grid.linewidth': 0.5,\n",
    "    'lines.linewidth': 2,\n",
    "    'lines.markersize': 8,\n",
    "})\n",
    "\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "metric_labels = ['Faithfulness', 'Answer Relevancy', 'Context Precision', 'Context Recall']\n",
    "available_metrics = [m for m in metrics if m in df_results.columns]\n",
    "available_labels = [metric_labels[i] for i, m in enumerate(metrics) if m in df_results.columns]\n",
    "\n",
    "if not available_metrics:\n",
    "    raise ValueError(\"Nenhuma métrica esperada foi encontrada em df_results.\")\n",
    "\n",
    "plot_df = df_results[available_metrics].apply(lambda col: col.astype(float)).copy()\n",
    "mean_scores = plot_df.mean().tolist()\n",
    "n_queries = len(plot_df)\n",
    "\n",
    "colors = ['#2c3e50', '#3498db', '#e74c3c', '#27ae60'][:len(available_metrics)]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 42))\n",
    "fig.suptitle(\"Avaliação de Métricas RAGAS\", fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "ax1 = fig.add_subplot(7, 1, 1)\n",
    "bars = ax1.bar(range(len(available_metrics)), mean_scores, color=colors, edgecolor='black', linewidth=1, width=0.5)\n",
    "ax1.set_title(\"(a) Métricas Médias\", fontweight='bold', pad=12)\n",
    "ax1.set_ylabel(\"Score Médio\")\n",
    "ax1.set_ylim(0, 1.15)\n",
    "ax1.set_xticks(range(len(available_metrics)))\n",
    "ax1.set_xticklabels(available_labels)\n",
    "ax1.axhline(y=1.0, color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "ax1.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "\n",
    "for i, (bar, score) in enumerate(zip(bars, mean_scores)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, score + 0.03, f'{score:.3f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax2 = fig.add_subplot(7, 1, 2)\n",
    "x = np.arange(n_queries)\n",
    "width = 0.18\n",
    "multiplier = 0\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(available_metrics, available_labels)):\n",
    "    offset = width * multiplier\n",
    "    rects = ax2.bar(x + offset, plot_df[metric].values, width, label=label,\n",
    "                    color=colors[i], edgecolor='black', linewidth=0.6)\n",
    "    \n",
    "    for rect, val in zip(rects, plot_df[metric].values):\n",
    "        height = rect.get_height()\n",
    "        ax2.text(rect.get_x() + rect.get_width()/2, height + 0.02, f'{val:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    multiplier += 1\n",
    "\n",
    "ax2.set_title(\"(b) Métricas por Query\", fontweight='bold', pad=12)\n",
    "ax2.set_xlabel(\"Query\")\n",
    "ax2.set_ylabel(\"Score\")\n",
    "ax2.set_xticks(x + width * (len(available_metrics) - 1) / 2)\n",
    "ax2.set_xticklabels([f\"Q{i+1}\" for i in range(n_queries)])\n",
    "ax2.set_ylim(0, 1.25)\n",
    "ax2.axhline(y=1.0, color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "ax2.legend(loc='upper right', frameon=True, framealpha=0.95, fontsize=9, ncol=4)\n",
    "ax2.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "\n",
    "ax3 = fig.add_subplot(7, 1, 3, projection='polar')\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(available_metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "radar_values = mean_scores + [mean_scores[0]]\n",
    "\n",
    "ax3.plot(angles, radar_values, color='#2c3e50', linewidth=2.5, marker='o', markersize=10)\n",
    "ax3.fill(angles, radar_values, color='#3498db', alpha=0.25)\n",
    "\n",
    "# Labels com valores\n",
    "for angle, value, label in zip(angles[:-1], mean_scores, available_labels):\n",
    "    ax3.annotate(f'{value:.3f}', xy=(angle, value), xytext=(angle, value + 0.18),\n",
    "                 ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax3.set_xticks(angles[:-1])\n",
    "ax3.set_xticklabels(available_labels, fontsize=11)\n",
    "ax3.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax3.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9, color='gray')\n",
    "ax3.set_ylim(0, 1.15)\n",
    "ax3.set_title(\"(c) Radar de Métricas\", fontweight='bold', pad=20, y=1.1)\n",
    "ax3.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "ax4 = fig.add_subplot(7, 1, 4)\n",
    "heatmap_data = plot_df.values\n",
    "im = ax4.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "ax4.set_title(\"(d) Heatmap de Scores\", fontweight='bold', pad=12)\n",
    "ax4.set_xticks(np.arange(len(available_metrics)))\n",
    "ax4.set_xticklabels(available_labels)\n",
    "ax4.set_yticks(np.arange(n_queries))\n",
    "ax4.set_yticklabels([f\"Q{i+1}\" for i in range(n_queries)])\n",
    "\n",
    "for i in range(n_queries):\n",
    "    for j in range(len(available_metrics)):\n",
    "        val = heatmap_data[i, j]\n",
    "        text_color = 'white' if val < 0.4 or val > 0.75 else 'black'\n",
    "        ax4.text(j, i, f\"{val:.2f}\", ha='center', va='center',\n",
    "                 color=text_color, fontsize=12, fontweight='bold')\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax4, fraction=0.03, pad=0.02)\n",
    "cbar.set_label(\"Score\", fontsize=11)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "ax5 = fig.add_subplot(7, 1, 5)\n",
    "box_data = [plot_df[m].dropna().values for m in available_metrics]\n",
    "bp = ax5.boxplot(box_data, labels=available_labels, patch_artist=True, \n",
    "                  showmeans=True, meanprops={'marker': 'D', 'markerfacecolor': 'red', 'markersize': 8})\n",
    "\n",
    "for i, (patch, color) in enumerate(zip(bp['boxes'], colors)):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "    patch.set_edgecolor('black')\n",
    "    patch.set_linewidth(1)\n",
    "\n",
    "for i, data in enumerate(box_data):\n",
    "    mean_val = np.mean(data)\n",
    "    median_val = np.median(data)\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    \n",
    "    ax5.text(i + 1, q3 + 0.12, f'μ={mean_val:.2f}', ha='center', va='bottom', \n",
    "             fontsize=10, fontweight='bold', color='red')\n",
    "\n",
    "    ax5.text(i + 1, q1 - 0.12, f'M={median_val:.2f}', ha='center', va='top', \n",
    "             fontsize=10, color='black', fontweight='bold')\n",
    "\n",
    "ax5.set_title(\"(e) Distribuição de Scores\", fontweight='bold', pad=12)\n",
    "ax5.set_ylabel(\"Score\")\n",
    "ax5.set_ylim(-0.3, 1.35)\n",
    "ax5.axhline(y=1.0, color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "ax5.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "\n",
    "ax6 = fig.add_subplot(7, 1, 6)\n",
    "markers = ['o', 's', '^', 'D']\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(available_metrics, available_labels)):\n",
    "    line, = ax6.plot(x, plot_df[metric], marker=markers[i], label=label,\n",
    "                     color=colors[i], linewidth=2.5, markersize=10, markeredgecolor='black', markeredgewidth=0.8)\n",
    "    # Adiciona valores em cada ponto\n",
    "    for xi, val in zip(x, plot_df[metric]):\n",
    "        ax6.annotate(f'{val:.2f}', xy=(xi, val), xytext=(0, 12), textcoords='offset points',\n",
    "                     ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax6.set_title(\"(f) Evolução por Query\", fontweight='bold', pad=12)\n",
    "ax6.set_xlabel(\"Query\")\n",
    "ax6.set_ylabel(\"Score\")\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels([f\"Q{i+1}\" for i in range(n_queries)])\n",
    "ax6.set_ylim(-0.05, 1.3)\n",
    "ax6.axhline(y=1.0, color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "ax6.legend(loc='lower right', frameon=True, framealpha=0.95, fontsize=10, ncol=2)\n",
    "ax6.grid(linestyle=':', alpha=0.4)\n",
    "\n",
    "ax7 = fig.add_subplot(7, 1, 7)\n",
    "ax7.axis('off')\n",
    "\n",
    "desc = plot_df.describe().loc[['mean', 'std', 'min', 'max']].round(3)\n",
    "\n",
    "# Criar tabela\n",
    "table_data = []\n",
    "table_data.append(['Métrica', 'Média (μ)', 'Desvio (σ)', 'Mínimo', 'Máximo'])\n",
    "for m, label in zip(available_metrics, available_labels):\n",
    "    table_data.append([\n",
    "        label,\n",
    "        f'{desc.loc[\"mean\", m]:.3f}',\n",
    "        f'{desc.loc[\"std\", m]:.3f}',\n",
    "        f'{desc.loc[\"min\", m]:.3f}',\n",
    "        f'{desc.loc[\"max\", m]:.3f}'\n",
    "    ])\n",
    "\n",
    "table = ax7.table(\n",
    "    cellText=table_data[1:],\n",
    "    colLabels=table_data[0],\n",
    "    loc='center',\n",
    "    cellLoc='center',\n",
    "    colColours=['#f0f0f0'] * 5,\n",
    "    cellColours=[['white'] * 5 for _ in range(len(available_metrics))]\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1.2, 2)\n",
    "\n",
    "# Estilizar cabeçalho\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_text_props(fontweight='bold')\n",
    "    table[(0, i)].set_facecolor('#d0d0d0')\n",
    "\n",
    "ax7.set_title(\"(g) Resumo Estatístico\", fontweight='bold', pad=12, y=0.95)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
